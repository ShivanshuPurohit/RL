{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Beta\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from agent import Agent, img_stack\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "seed = 0 \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "action_repeat = 10\n",
    "env = gym.make('CarRacing-v0', verbose=0)\n",
    "state = env.reset()\n",
    "print('env.action_space.shape: ', env.action_space.shape)\n",
    "reward_threshold = env.spec.reward_threshold\n",
    "print('reward_threshold', reward_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show what a preprocessed image looks like\n",
    "frame, _, _, _ = env.step(np.array([2., 1., 1.]))\n",
    "\n",
    "print('frame.shape: ', frame.shape)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "#-------------------------------#\n",
    "\n",
    "def rgb2gray(rgb, norm=True):\n",
    "        # rgb image -> gray [0, 1]\n",
    "    gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "    if norm:\n",
    "        # normalize\n",
    "        gray = gray / 128. - 1.\n",
    "    return gray\n",
    "\n",
    "img_gray = rgb2gray(frame)\n",
    "\n",
    "#-------------------------------# \n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "print('img.shape: ', img_gray.shape)\n",
    "\n",
    "# 96 x 96 black and white image\n",
    "plt.imshow(img_gray, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class  Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper():\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env  \n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "\n",
    "        self.die = False\n",
    "        img_rgb = env.reset()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * img_stack  # four frames for decision\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(action_repeat):\n",
    "            img_rgb, reward, die, _ = env.step(action)\n",
    "            # don't penalize \"die state\"\n",
    "            if die:\n",
    "                reward += 100\n",
    "            # green penalty\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "            total_reward += reward\n",
    "            # if no reward recently, end the episode\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "            if done or die:\n",
    "                break\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == img_stack\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        # record reward for last 100 steps\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(agent, directory, filename, suffix):\n",
    "    torch.save(agent.net.state_dict(), '%s/%s_%s.pth' % (directory, filename, suffix))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(device)\n",
    "\n",
    "env_wrap = Wrapper(env)\n",
    "\n",
    "limits = [200, 400, 600, 800, 830, 870, 900]\n",
    "\n",
    "def return_suffix(j):\n",
    "    suf = '0'\n",
    "    for i in range(len(limits)-1):\n",
    "        if j > limits[i] and j < limits[i+1]:\n",
    "            suf = str(limits[i+1])\n",
    "            break\n",
    "        \n",
    "        i_last = len(limits)-1    \n",
    "        if  j > limits[i_last]:\n",
    "            suf = str(limits[i_last])\n",
    "            break\n",
    "    return suf           \n",
    "\n",
    "\n",
    "def ppo_train(n_episodes=5000, save_every=500):\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    avg_scores_array = []    \n",
    "\n",
    "    timestep_after_last_save = 0\n",
    "    \n",
    "    time_start = time.time()\n",
    "\n",
    "    running_score = 0\n",
    "    state = env_wrap.reset()\n",
    "    \n",
    "    i_lim = 0\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        \n",
    "        timestep = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        ## score = 0\n",
    "        state = env_wrap.reset()\n",
    "\n",
    "        while True:    \n",
    "            \n",
    "            action, a_logp = agent.select_action(state)\n",
    "            next_state, reward, done, die = env_wrap.step( \n",
    "                action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "\n",
    "            if agent.store((state, action, a_logp, reward, next_state)):\n",
    "                print('updating')\n",
    "                agent.update()\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            timestep += 1  \n",
    "            timestep_after_last_save += 1\n",
    "            \n",
    "            if done or die:\n",
    "                break\n",
    "                \n",
    "        running_score = running_score * 0.99 + total_reward * 0.01\n",
    "\n",
    "        scores_deque.append(total_reward)\n",
    "        scores_array.append(total_reward)\n",
    "\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        avg_scores_array.append(avg_score)\n",
    "        \n",
    "        s = (int)(time.time() - time_start)        \n",
    "        print('Ep. {}, Ep.Timesteps {}, Score: {:.2f}, Avg.Score: {:.2f}, Run.Score {:.2f}, \\\n",
    "Time: {:02}:{:02}:{:02} '\\\n",
    "            .format(i_episode, timestep, \\\n",
    "                    total_reward, avg_score, running_score, s//3600, s%3600//60, s%60))  \n",
    "       \n",
    "        \n",
    "        # Save episode if more than save_every=500 timesteps\n",
    "        if timestep_after_last_save >= save_every:\n",
    "\n",
    "            timestep_after_last_save %= save_every            \n",
    "            suf = return_suffix(avg_score)\n",
    "            save(agent, 'dir_chk', 'model_weights', suf)\n",
    "            \n",
    "        if np.mean(scores_deque) > reward_threshold:\n",
    "            print(\"Solved environment! Running score is {:.2f}, Avg.Score: {:.2f} !\" \\\n",
    "                  .format(running_score, avg_score))\n",
    "            break\n",
    "            \n",
    "    return scores_array, avg_scores_array    \n",
    "            \n",
    "scores, avg_scores  = ppo_train()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(agent, 'dir_chk', 'model_weights', '1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
